[{"authors":["Sudeepa Nadeeshan"],"categories":null,"content":"I\u0026rsquo;m a risk taking , motivated , passionate Computer Science and Engineering graduate who loves to develop innovative solutions and enhance technical skills . I\u0026rsquo;m Interested in Software Development, Machine Learning, Data Science and Intelligent Transport Systems. An adaptable person who is willing to give a maximum contribution to the team and organization to achieve its objectives.\n","date":1588204800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588204800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a risk taking , motivated , passionate Computer Science and Engineering graduate who loves to develop innovative solutions and enhance technical skills . I\u0026rsquo;m Interested in Software Development, Machine Learning, Data Science and Intelligent Transport Systems. An adaptable person who is willing to give a maximum contribution to the team and organization to achieve its objectives.","tags":null,"title":"Sudeepa Nadeeshan","type":"authors"},{"authors":[],"categories":null,"content":" To Be Updated Soon! ","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"514b59d0f9b0a1dbed0c6c10935889a8","permalink":"/talk/events/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/events/","section":"talk","summary":"Summary of the professional Events","tags":[],"title":"Events","type":"talk"},{"authors":[],"categories":null,"content":" The Cajon I made for myself. :D #ISajje @ Interblocks No Comments Trek to Adara Kanda Trek to Wangedigala Trek to Gawarawila Plains Trek to Thotupala Kanda @ The Fund Raising Programme for the Apeksha Cancer Hospital Ananda Mehewara 2019 We, Old anandian Engineering graduates helped two rural schools in Galgamuwa are.\n We conducted a lecture series about mathematics. Built an IT room Built Two outdoor class rooms. Donated school Material/ Sports Material Partitioned class rooms.  With the help of the fellow students, friends and the families.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"9e492de020d2c70bd9eaee14be10d832","permalink":"/talk/myhobbies/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/myhobbies/","section":"talk","summary":"More about Sudeepa","tags":[],"title":"My Hobbies","type":"talk"},{"authors":["Sudeepa Nadeeshan"],"categories":["DS"],"content":"The article has been posted on Medium Platform. It has two parts. Please read,\n Part 1 - Helping a Gym Rat to choose a Neighbourhood to live in Manhattan - Part¬†1 Part 2 -Helping a Gym Rat to choose a Neighbourhood to live in Manhattan - Part¬†2  ","date":1588204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588204800,"objectID":"8672794e69d15d056481a78706da68d6","permalink":"/post/ds/","publishdate":"2020-04-30T00:00:00Z","relpermalink":"/post/ds/","section":"post","summary":"Data Analysis with Python","tags":["DS, Python, ML"],"title":"Helping a Gym Rat to choose a Neighbourhood to live in Manhattan","type":"post"},{"authors":["Sudeepa Nadeeshan"],"categories":["Demo"],"content":"This is the third blog that I\u0026rsquo;m running as a computer professional. :D (Hope this will be the last one. Migrations are not that smooth). Let me briefly explain how I ended up here. First, I started a blog on Blogspot back in 2017, and It was cool. It was (http://sudeepanadeeshan.blogspot.com/. I was not happy with the way that blog look. That time I got to know about Wordpress. So I planned to move to Wordpress, but this poor student did not have enough money (or wasn\u0026rsquo;t willing to pay pocket money to some hosting company). Obviously, I checked for free services. Finally, I came across 000webhosting. Excellent, Cpanel hosting service with a free Tier. I jumped in. Searched for hundreds of free appropriate themes and finally satisfied with Me from themehall.\nI customized the theme in for my requirement. Added tweeter handle, coding plugins, blah blah. The blog was totally fine. You can check it from web.archive.org. Everything was fine until 000webhost suddenly decided to delete my website from there servers without keeping ANY BACKUPS! I even tried to pay for the past months and get the site back. NOTHING WORKED! Actually, I should not blame them for not keeping the backups as it is my duty to baking up my own website (but come on, you should keep something for the customers!). So that\u0026rsquo;s how I said goodbye to Wordpress.\nSo I wanted to develop my blog again. But I was not particularly eager to use any blogging platforms like Medium (As I do know something about coding ;) ). I did some research about the available blogging platforms for coders and heard about Hugo. Hugo has several themes and widgets. I found Academic theme very attractive among them.\nAcademic has been released under the MIT license. George Cushen lists key features of Academic as follows,\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  ","date":1576800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576800000,"objectID":"6ef85d38bfd0333860a4ace6607b9871","permalink":"/post/what_happend_to_previous_blog/","publishdate":"2019-12-20T00:00:00Z","relpermalink":"/post/what_happend_to_previous_blog/","section":"post","summary":"blogspot - Wordpress - Hugo.","tags":["Academic"],"title":"Why Your Own Blog?","type":"post"},{"authors":["Sudeepa Nadeeshan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2cdd19c4cab4d5953af57498909260b0","permalink":"/publication/crowdsourcing/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/crowdsourcing/","section":"publication","summary":"Speech corpora do not exist for most low-resource languages. Thus, creating speech corpora for a language of such a nature is challenging and involves a significant amount of time and effort.","tags":["data corpus","data collection tool","low resourced languages","NLP"],"title":"Voicer: A Crowd Sourcing Tool for Speech Data Collection","type":"publication"},{"authors":["Sudeepa Nadeeshan"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"An example preprint / working paper","type":"publication"},{"authors":["Sudeepa Nadeeshan"],"categories":["Demo"],"content":" (Note:- The post was migrated from the previous blog written on 24th September 2018 web.arvhive.org). That was a lossy migration and the images were not be able to recover using webarchive. See What Happend to the previous blog \nAfter playing with the homemade Cajon which I built (of cause father helped üòáüòá ),I eagerly wanted to have an acoustic drum set (This dream is there since Day 1).\nThis the Cajon that I built (The righter image shows the Cajon before polishing)\nWell , I found a great wholesale seller from Alibaba and he agreed to send a single set for me (If you are interested contact me I can give you his WeChat contact). All seemed well until I realized the shipping prices and taxes are too high for a single setüòë. The government applies a tax around 45% for a set üòë . That moment I stopped dreaming about a brand-new set. Poor me then started checking second-hand products from Ikman. Most of the times all the ads are about electronic drums üôÅ . But a lot of cool deals comes to Ikman and vanishes so quickly. Most of the times I forget to check Ikman regularly . This was the time that I was taking Data Mining \u0026amp; Information Retrieval and Machine Learning for semester seven (here are spiders that I developed the crawl newsfirst.lk ). So, I mixed everything and built a ML model to identify whether the new ad is personally matches my requirement. Here are the main things I did.\n Scraping the existing ads. Pre-processing the scraped data. Building the ML model and train it. Developing pipeline for classifying a new ad in real time.  1. Scraping the existing ads. Here I Scraped all the adds related to drums which are currently available at Ikman (not a much ads were available as ikman is deleting each ad after 60 days). Well I used the knowledge that I gained from Data Mining and Information Retrieval to do this. I scraped around 600 ads from the site and saved them as json/csv. The following attributes are in an ads.csv.\n Price ‚Äì price of the drum item Title ‚Äì title of the ad Link ‚Äì URL of the advertisement. Details ‚Äì description the seller has given. Location ‚Äì Item location.  I used Python library called scrapy to scrape the ads. There are very nice tutorials available for learning scrapy and even the official documentation is easy to understand. In scrapy you can define ‚Äòspiders‚Äô who crawl the webpages for you. I created a spider called ‚Äúikman‚Äù to crawl all the adds related to drums and drum items.\nHere is a sample of the webpage in ikman you also follow this link to see the latest ads üòõ\nI wrote the following spider to get all the ads.\n I would like to point out 2 main things about this spider.\nI. It was required to get to page by page to crawl all the ads. The ‚Äòfor loop‚Äô in line 14 is to loop and find every ad items in the crawled web page. The ads are in div class of ‚Äúui-item‚Äù. Here I have used CSS to extract the elements in the page (you may need to use ‚ÄòXPATH‚Äô instead of CSS when you need extract the absolute path to the element e.g.- line 31). Line 23 -25 is used to go to the next page when the crawling of the current page is over.\nII. It was required to go to inside of each ad to get the ad details. If you watch the web page closely you would see that in order to get the ad description you need to click the ad link. To do that, We send another request to the extracted ‚Äòlink‚Äô of the ad(line 20 -21). That response is separately handled in parse_next function.\nIn addition to that I would like to mention the following facts which could help you when building a spider.\nI. Setting encoding format. This sets the encoding method of the output file. In my case some ads are containing local language ‚ÄúSinhala‚Äù and some weird emojis (Marketing Things üòÖüòÖ). If you want to capture these properly you have to set the following settings in the settings.py of your scrapy project.\nFEED_EXPORT_ENCODING = \u0026quot;utf-8\u0026quot;  II. Don‚Äôt check your output CSV (if you decide to CSV as the output file format) using ms excel. I got into this trap and thought I have done something wrong while scraping. Non Unicode scrapped data are not shown properly MS Excel. You would see some senseless characters if you open it in Excel. Better try a code editor like Visual Studio Code (CSV is nothing but a just tabular format of keeping data. The columns are separated by commas and the rows are separated using newlines).\nYou can run following command to scrape the data. (Please keep those instructions in mind üòÄ)\nscrapy crawl ikman -o scrapedData.csv  2 Pre-processing the scraped data. price,title,link,details,location \u0026quot;Rs 17,500\u0026quot;,SABIAN PRO High hat 14 inches pair,https://ikman.lk/en/ad/sabian-pro-high-hat-14-inches-pair-for-sale-colombo,SABIAN PRO High hat pair 14 inches . superb condition.not used in Srilanka,Colombo \u0026quot;Rs 64,000\u0026quot;,ROLAND SPD SX,https://ikman.lk/en/ad/roland-spd-sx-for-sale-anuradhapura,Brand new.,Anuradhapura \u0026quot;Rs 16,500\u0026quot;,PAISTE 101 High Hat 14 inch,https://ikman.lk/en/ad/paiste-101-high-hat-14-inch-for-sale-colombo,Paiste 101 hihat pair. not used in Srilanka.Imported from Japan,Colombo \u0026quot;Rs 160,000\u0026quot;,Drum kit ( Pearl ),https://ikman.lk/en/ad/drum-kit-pearl-for-sale-kalutara,Drum kit ( Pearl )8/10/12/16 toms.Hat 14,Kalutara \u0026quot;Rs 16,000\u0026quot;,LASER High hat 14 inch( Germany),https://ikman.lk/en/ad/laser-high-hat-14-inch-germany-for-sale-colombo,\u0026quot;Laser high hat pair , made in Germany. not used in Srilanka. Imported from Japan\u0026quot;,Colombo  First take a copy of a scrapedData.csv (called training.csv) and add another column. Called ‚Äúoutput‚Äù. Now add ‚ÄúY‚Äù or ‚ÄúN‚Äù to each entry in the file to this attribute. ‚ÄúY‚Äù means I‚Äôm interested in this add and no is no I‚Äôm not üòÄ. Well I was the boss here so I had to tag all the data üòë. Here is a sample of ‚Äòtraining.csv‚Äô.\nprice,title,link,details,location,output \u0026quot;Rs 17,500\u0026quot;,SABIAN PRO High hat 14 inches pair,https://ikman.lk/en/ad/sabian-pro-high-hat-14-inches-pair-for-sale-colombo,SABIAN PRO High hat pair 14 inches . superb condition.not used in Srilanka,Colombo, Y \u0026quot;Rs 64,000\u0026quot;,ROLAND SPD SX,https://ikman.lk/en/ad/roland-spd-sx-for-sale-anuradhapura,Brand new.,Anuradhapura, N \u0026quot;Rs 16,500\u0026quot;,PAISTE 101 High Hat 14 inch,https://ikman.lk/en/ad/paiste-101-high-hat-14-inch-for-sale-colombo,Paiste 101 hihat pair. not used in Srilanka.Imported from Japan,Colombo, Y \u0026quot;Rs 160,000\u0026quot;,Drum kit ( Pearl ),https://ikman.lk/en/ad/drum-kit-pearl-for-sale-kalutara,Drum kit ( Pearl )8/10/12/16 toms.Hat 14,Kalutara, N \u0026quot;Rs 16,000\u0026quot;,LASER High hat 14 inch( Germany),https://ikman.lk/en/ad/laser-high-hat-14-inch-germany-for-sale-colombo,\u0026quot;Laser high hat pair , made in Germany. not used in Srilanka. Imported from Japan\u0026quot;,Colombo, N  Now we have the data to train a model. I will be discussing about the model that I built to classify these advertisements in the next article. That is the most interesting part(After that I will discuss the pipelines that I developed to crawl live data and test them against the model using Scrapyd , Scrapyd-client, sendGrid on Bitnamy Hosting). Let me know anything if there is any unclear thing here. Wait for the Dark Art üòÄ !\n","date":1537747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537747200,"objectID":"630c1943999b75fca1a30ae15880745f","permalink":"/post/ikman/","publishdate":"2018-09-24T00:00:00Z","relpermalink":"/post/ikman/","section":"post","summary":"Scrappying to Machine Learning","tags":["CI/CD, Github, Codes Demo"],"title":"The only ML model I created for my own usage üòÄ (part 1 of 3)","type":"post"},{"authors":["Sudeepa Nadeeshan"],"categories":["Demo"],"content":"(Note:- The post was migrated from the previous blog written on 17th March 2018 web.arvhive.org) That was a lossy migration and the images were not be able to recover using webarchive. See what happend to the previous blog\nI along with Ciperlabs team (which is a very little start up, started with two of my friends) developed an app for a client which is extracting tweets of a special user. If I go bit deep, we used twitter API for getting the latest tweet of a specific user. In order to that we have to access the timeline of the user and filter the latest tweet among all the tweets.[see ‚Äì documentation]. As the document specifies the API allows only 900 requests in a window of 15 minutes with the user AUTH token. So, we needed to limit our system to send 900\u0026frasl;15*60 ‚Üí 1 request per second. Our server sends 1 requests per second to twitter side to read the latest tweet. We could have used twitter stream API easily, but it had many limitations, so we decided to go with our own implementation.\nBack to the main topic. Building a server (let‚Äôs say tweet reader) to grab the latest tweet was just a part of the main project the big picture was to build a transaction automation system. The client wanted to automate the option trade placing on tradestation trading platform. we deployed the application on Heroku for testing the functionality. As the Heroku free tier has a condition saying the app will be slept after 30 minutes of inactivity[here]. It was a really problematics for us to use this as a server which runs for 24*7.\nAfter bit of a research I could be able to find about AWS lambda. It is a serverless, event driven platform. Serverless shows the current evolution of the hosting applications. Early days if we need to host our application we had to host it by ourselves. Then shared hosting came up where each application must share allocated resources among each other‚Äôs. After that using a dedicated virtual machine on a cloud platform became popular. In this case you have to pay for the performance of the virtual machine that you are purchasing. What if you need to process your application for a very limited time like 2 seconds. If you buy a dedicated machine just for this task? it will be a waste of resources as most of the available commercial plans are either monthly basic or year basis. This is where serverless architecture comes in to play. Basically, you run your small small tasks on somebody else‚Äôs machine and pay for the time usage. AWS lambda is a good implementation of serverless architecture. Here my micro task is just to send a http request to my tweet reader server. If I got a dedicated VM or host the application by myself, I have to spend a lot of money. In this case I do it totally free.\nAs I had already created AWS student account I started working on Lamda immediately (To use AWS student account it was required to add a credit card). Lambda offers 1 million requests per month in their free tier (see ‚Äì Documentation). This free tier of Lamba was more than enough for our application. As Heroku app closes in each thirty minutes we have to awake the server in each 30 minutes. So we need only 2* 24 = 48 requests per day and maximum of 48*31 = 1488 per months among the 1 MN free request we could use. Cool.\nLet‚Äôs move to the actual works. After you log on to the AWS you can search ‚ÄúLambda‚Äù in AWS services.\nThen we can easily create a new lambda function by clicking ‚ÄúCreate Function‚Äù\nThe required details must put in the following form. I used following setting for the tutorial. After clicking ‚Äúcreate function‚Äù select ‚ÄúBlueprints‚Äù rather than selecting ‚ÄúAuthor from scratch‚Äù\nBy searching ‚Äúlambda-canary‚Äù we can find the template that matches for our requirement. By the time the tutorial was written there were 2 lambda-canary functions, one is in python 3 and other one is in python 2.7. I used python 2.7 for the task. After that you have to fill the basic information about the function like name and role.\nThen we must configure cloudwatch-events. It is required to follow syntax mentioned in the documentation to add the scheduler expression. The rate has set to 5 minutes here as you can see in Schedule Expression section and that follows the syntax of scheduler expression.\nThen we must enable the trigger after creating it.\nAfter successfully adding enabling the function we have to configure the sample canaray code.\nThat configuration can be easily done by setting the environment variables. The site will be the website that we are going to send the http requests. In my case it is Heroku app. You can add the expected String to be in the page for the ‚ÄúExpected‚Äù.\nAfter adding the relevant environment variables, you just have to create the function. Then you will be redirected to configuration page where you can still change the function. There is another tab called ‚ÄúMonitoring‚Äù. There you can see logs of your microservice.\nThis provides a various kind of logs on your application. ‚ÄúJump to matric‚Äù shows you a clear expansion of the matrix and ‚ÄúJump to logs‚Äù shows all the logs.\nThis is a basic example of using AWS Lambda. This powerful tool can be used to do lots of things. You can try the sample codes and get an understanding about the capabilities of that. Let‚Äôs meet with a new post soon.\n","date":1517356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517356800,"objectID":"4d97972dac132e6c32a6924fd9940f1e","permalink":"/post/lambda/","publishdate":"2018-01-31T00:00:00Z","relpermalink":"/post/lambda/","section":"post","summary":"CI/CD with Github Webhooks","tags":["AWS Labda, Heroku"],"title":"Keeping Free Heroku app awake for all the time","type":"post"},{"authors":["Sudeepa Nadeeshan"],"categories":["Demo"],"content":" (Note:- The post was migrated from the previous blog written on 17th November 2017 web.arvhive.org). That was a lossy migration and the images were not be able to recover using webarchive. See what happend to the previous blog\nAfter working with Jenkins at Interblocks, I really wanted to try something similar continuous integration tool for works. What is Continuous Integration?. The name itself defines what it is. It is basically a methodology used by the developers when they are working as a team. Whenever a team member commits a change to the repository that they are using, we can verify the new commit by an automated build. We can run some tests on that automated build using scripts. That‚Äôs the simplest idea. It will help to identify the mistakes and errors done by the programmers before merging to the branch. Ok fine. Then why TravisCI over jenkins. Though Jenkins seems cool and familiar, as I‚Äôm not usually working with JavaEE in daily basis, I wanted to do something related to nodeJs. Is that the only reason?. Not exactly actually Node project can be built with jenkins. But TravisCI is prefered over Jenkins based on some reasons and I‚Äôve listed them below. In my case I got a Free Ticket :D. Student are given the facility to use TravisCI commercial version under the Github Student Pack. Github with the collaboration of some of the industry giants, provides a really good opportunity to try the paid tools free. Here is the link and give it a try. I have used Github private repo pack, Namecheap domain pack, digital Ocean pack and planning to use Strip with a future projects.(Also there is another CI tool called Bamboo by atlassian and they are also providing it under their classroom package. Honestly I‚Äôve never Tried it yet).\nTravis CI and Jenkins.    Jenkins TravisCI     Suits for large project Suits for open source projects   Free Pro version has more features   Dedicated server is needed(You have to host it ) No need of dedicated servers(It is already hosted)   Configuring takes time No need of dedicated servers(It is already hosted)    # TravisCI\nTravis CI is a hosted continuous integration and deployment system. As I mentioned before you do not need to host it by yourself you can use it as a SaaS. Like WordPress.com and WordPress.org,There are two versions of it, travis-ci.com (Travis Pro) for private repositories, and travis-ci.org for public repositories.\nBasics To build a project using TravisCI we have to have a .travis.yml in our repository. Basically that includes the metadata about the project as well as the scripts that we are hoping to run to test after building the branch. I will show you a one in the demo. Let‚Äôs drive bit deep. There are few common terms in TravisCI and it‚Äôs better to be familiarise with them. A deployment is called a job which has several intermediate, sequential tasks called phases and the main phases would be install,script,deploy those also will be covered in the demo. There is another term called ‚ÄúBuild‚Äù which implements a group of tasks. To understand ‚ÄúStage‚Äù term have a look at the below image.\nFigure ‚Äì Stage\n### Process\nFirst creating a virtual environment for the app. Second cloning the repository into the virtual environment. Build the project. Run the test cases.\n### Supporting Languages\nTravisCI supports a range of languages. Here are some of them. Julia, Objective-C, Perl, Perl6, PHP, Python, R, Ruby, Rust, Scala, Smalltalk, Visual Basic.Android, C, C#, C++, Clojure, Crystal, D, Dart, Erlang, Elixir, F#, Go, Groovy Haskell, Haxe, Java, JavaScript (with Node.js).\n# DEMO\nI‚Äôm just trying to deploy a simple nodeJS application using TravisCI. For this I‚Äôm going to use travis-ci.com. As I mentioned before, first of all I have to write a .travis.yml to my repo and commit it . This is my sample yml file\n.travis.yml\n If there is not such a command TravisCI will automatically run ‚Äúnpm test‚Äù command which is specified in the ‚Äúpackage.json‚Äù file. This command will be created automatically when you are initiating npm. This is it\nFigure ‚Äì Package.json\nSo far so good but why It is not building :/ .After doing a bit diving, I found the reason. Whenever a phase returns a non zero exit code, the TravisCI will think that as an ‚Äú√ãrror‚Äù . The default code is ‚Äú‚Äù1‚Äù, So we have to change it to ‚Äú0‚Äù. So here is my final result.\nFigure ‚Äì Build Success\nHooorayyyyyyyyyyyyyyyyyyyyyyyyyyyy\nSo it succeeded, But by changing only the value to ‚Äú1‚Äù, can we guarantee that the project built successfully? No for that you have to run real test cases. But for this one I did something else. I wrote a script in travis.yml to start the node server.\nFigure ‚Äì Running the node server.\nThe server started perfectly(that means the project is successfully built) but did not get the green sign. It was running for around 10 minutes and gave a build failure. Actually this failure doesn‚Äôt mean that we did something wrong. Just running the server will not give neither 1 nor 0 at the end of the execution. So t he test runs for the previously configured time and then stops after getting a timeout. That why it get failed. So we can be happy for now.\nImportant facts When you do a change in github, sometimes it won‚Äôt sync with travisCI for manual building. For this reason I recommend to sync manually every time when you do a change to the code For that follow below steps.\nIn the left corner tap the plus sign\nFigure ‚Äì Select Syncing\nThen tap the syncing button(IF you hover over you will see the last synced time)\nFigure ‚Äì Sync\nCool features Adding to build status You may have seen some badges in the Readme files in some Github repositories saying the build is passed. It gives some kind of good vibes to the person who is going to use your code.\nFigure Build Passing tag in readme.md\nTo add this. First tap on the build sign on the top to the page\nFigure ‚Äì On the top of the project\nThen you get the image links related to your build.\nFigure ‚Äì Image links related to the build\nGo to the readme.md and edit the link as follows.\n Verifying the commits. Wait a second. Why were we going to use CI in the first place??. To automate the build right? Do you see anything automated here. No. That is we did this for just for a demo. Actually each time when you are committing Travis CI will automatically build them. Actually this manually building that we just did is a beta feature that Travis is providing. If you select any file in your Github repo and go it‚Äôs its history you will see something like this.\nFigure -Auto building\nIt will show the end result of your builds. Saying whether they passed or not. These will be available for every pull request too. So you can be sure about the code that you are going to merge to your repository. That‚Äôs all for today. Will meet with a new article soon. Bye.\n","date":1510876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510876800,"objectID":"d97427d05bea2a114da23fc41cc8ff18","permalink":"/post/ci-cd-compare/","publishdate":"2017-11-17T00:00:00Z","relpermalink":"/post/ci-cd-compare/","section":"post","summary":"CI/CD, Travis","tags":["CI/CD, Travis"],"title":"Continuous Integration with Travis CI","type":"post"},{"authors":["Sudeepa Nadeeshan"],"categories":["Demo"],"content":" (Note:- The post was migrated from the previous blog written on 26th August 2017 web.arvhive.org). That was a lossy migration and the images were not be able to recover using webarchive. See what happend to the previous blog\nHi folks. I got to dive into a totally different Library last week. It is non other than OpenCV . While I was working as an intern at interblocks one of my tech leads said he never tried to stream a video between two machines, Actually he wanted to transfer the stream only when someone is in front of the computer. It is not a work relate one, but just a random idea. So I wanted to give it a try. Here is the basic diagram.\nAs the Face Detector and End User are talking using REST API I thought to use Django as the web frame work and openCV as the face detection library. I‚Äôve never used both of them before. So I decided to go with those as a learning task.\nDjango is a high-level Python Web framework. This is the basic setup for a app in Django. I followed the official documentation . Here is a highlight of it.\nInstall Python. Install a Python Virtual Environment. To install the package manager Download get-pip.py\nGo to folder containing get-pip.py and run\npython get-pip.py  Installing the virtual environment pip install virtualen  What virtual environment do is it create a virtual system which is almost like the normal system and it prevents any harm that could be caused because of the programme development The virtual environment. Here you can find an explanation about the importance of the virtual environment very clearly, just have a look at it.\nCreating the Project Folder mkdir env  Go to the folder and create a new project cd env Virtualenv mysite  Active the newly created environment cd env\\mysite\\scripts\\activate  This will change the command line as follows.\n(env)C:\\Users\\username\\Documents\\mysite\u0026gt;  Now the environment is up and running we can develop ord project on top of that\nInstall Django. This will install the latest version of django and you can specify the version by,\npip install django==x.y.z  You can cross check the installation by running the python interpreter here and checking the python version here.\n\u0026gt;\u0026gt;\u0026gt;Python \u0026gt;\u0026gt;\u0026gt;Import Django dJ \u0026gt;\u0026gt;\u0026gt;DJ.get_version()  Back to the topic. Now we have to starting a new Project. A project can hold multiple apps FYI üòÄ\ndjango-admin startproject mysite  This will automatically create a new folder with the following structure.\nmysite/ manage.py mysite/ __init__.py settings.py urls.py wsgi.py  As Django automatically creates some applications by defaults like Admin Programme , User Management , Authentication we have to set up a database in order to work with them.\nDirect to mysite. And run\npython manage.py migrate  again to make sure that you are in the newly created project folder. In ‚Äúmysite‚Äù you can easily use CD command (in case you are confused with the running environment).\nAll set!!! Now you can run the server you just created.\npython manage.py runserver  If you want to change the server,\npython manage.py runserver 8080  If you have done everything right it will look like\nCreating your first app. Go to the project directory where your manage.py is and run the following command.\npython manage.py startapp myfirstapp  Testing the app As the django Doc says,\nIn views.py\nfrom django.http import HttpResponse def index(request): return HttpResponse(\u0026quot;Hello, world. You're at the polls index.\u0026quot;)  And in url.py\nfrom django.conf.urls import url  Then go back to the the project (mysite ) in this case, /mysite/urls.py\nfrom django.conf.urls import include, url from django.contrib import admin urlpatterns = [ url(r'^myfirstapp/', include('myfirstapp.urls')), url(r'^admin/', admin.site.urls), ]  This is the very basic set up of a Django app. I will share the web app for the solution using Django in the next post.\n","date":1503705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503705600,"objectID":"fd7a79ee386314875d80caa3bf1ea9d8","permalink":"/post/django/","publishdate":"2017-08-26T00:00:00Z","relpermalink":"/post/django/","section":"post","summary":"Django","tags":["Codes Demo"],"title":"Face Detection, Video Streaming with Python, OpenCV, Django ‚Äì Part 1","type":"post"},{"authors":["Sudeepa Nadeeshan"],"categories":["Demo"],"content":" (Note:- The post was migrated from the previous blog written on 17th March 2018 web.arvhive.org). That was a lossy migration and the images were not be able to recover using webarchive. See What Happend to the previous blog \nLast few days I spent digging into Continuous integration and delivering. I also have published a post about CI with Travis CI before. This time I wanted to run my own CI system. Why? Well that Idea came to me when we were developing a project in university. We had to develop an app to collect voice samples in Sinhala language from the public for the project and we developed a simple responsive mobile app for that. You can find it here.(yeah it is a simple app, why the hack you made it https ? it a simple app!! :D unfortunately we had to get an SSL to use microphone via the browser otherwise browsers won‚Äôt let us to use the mic. I used my free student SSL provided by Namecheap under Github Student Pack Promotion). I wrote the back end and it had only few functionalities (saving a voice sample on the Yandex drive and the local server) and they had been well tested. The front-end made the trouble here (yes yes front ends are trouble makers üòÄ ). It was a react app. We developed the application while it was running on the server. While we were doing QA we had to changes every time. whenever we found a new bug/improvement we needed to,\n Commit the change to Github. Log on to the server using Putty. Change the current directory. Pull the repository. Build the react build Restart the back end  Doing this for a simple typo was really annoying. :/ So I wanted to AUTOMATE it! While I was reading about Travis CI and Ansible (a Deployment automation tool) I realized the important of Webhooks. Basically it is an HTTP callback which notify to an endpoint when an event occurs. Github also has this facility.\nFor the web app we all contributed was not owned by me. So I had to use one of my own repo. I used a react application I‚Äôve hosted on Github called youtubeDownloader ( It is a simple application which is used to save youtube video files in Mega clould‚Äì this give free 50GB storage with a super encryption facility). For the testing I used my droplet on DigitalOcean which I bought using free credit that I got fromGithub Student Pack Promotion. My server is a Ubuntu 16.04.3 having 1GB RAM. Back to the topic. Here is overall architecture.\nThe started configuring the webhook first. You can configure this in Settings √† Webhook √† Add Webhook .(password confirmation step will be added here.\nThen you will have to configure the following settings.\nPayload URL ‚Äì This is the endpoint of your listening server. My one was in following format ‚Äì http://myip:8081/api/newPull\nContent type ‚Äì This sets the format of the content you are receiving. This can be either application/json or application/x-www-form-urlencoded. I personally prefer application/json as it is really easy to access.\nSecret ‚Äì This a really important and I will explain the important with the implementation. Just think this is some kind of an encryption key. Basically all the data will be encrypted using the key.\nThe I chose ‚Äújust the push event‚Äù as I only needed to automate things when a push happens. Also do not forget to tick the ‚ÄúActive‚Äù checkbox.\nNow we have to configure the server to listen for event notifications and the entered end point in the server.\nHere is the server. It is just a simple Nodejs server running on ubuntu. The following endpoint is the one that the pull request is sent by Github.\nrouter.post('/newPull',function(req,res){ .... }  Now the basics of cryptography comes in to the play. Remember the ‚Äúsecret‚Äù that you entered on Github repo? The requset data comes from Github is encrypted by the using HMAC . Github takes SHA1 hash function and the ‚ÄúSecret‚Äù you entered and the response body data to generate HMAC digest.This encryption helps to protect data integrity and the verify authentication. Github sends the HMAC they calculated as the field ‚ÄúX-Hub-Signature‚Äù inside the post request header. We must implement a way to regenerated the HMAC signature from our side too using post request body, secret key and sha1 hash function. If those to matches we can verify the request.\nI created .env file containing the session secret as follows. Then added .env to gitignore file to stop it is committed to github repository.\nSECRET_TOKEN=myToken }  Installing ‚Äòdotenv‚Äô node module helps to import environment variables in nodeJs. We just have to import it as,\nrequire('dotenv').config() }  We can access them as\nprocess.env.SECRET_TOKEN }  This is how we can implement HMAC generation from our side.\nrouter.post('/newPull',function(req,res){ var hmac, signature; //configuring hash function using sha1 and secret. hmac = crypto.createHmac(\u0026quot;sha1\u0026quot;, process.env.SECRET_TOKEN); //generating hash using request body hmac.update(JSON.stringify(req.body)); // format it to github format signature =\u0026quot;sha1=\u0026quot;+hmac.digest(\u0026quot;hex\u0026quot;); //validating if(signature===req.headers['x-hub-signature']){ //both the signatures match continue CI/CD CI.builder(res); } else{ res.sendStatus(200); } }); }  The we can run our server and test configuration. You can see a ‚ÄúRecent Delivery‚Äù section under your webhook on Github. This tells whether your end point responded to the webhook post request or not. If yes you can see a 200 -ok response with green colour. If there is something wrong it will get red.\nYou can use ‚ÄúRedeliver‚Äù to deliver the payload again for testing.\n(Please consider the time out of the github request is low. So you have to take care of the responding to github request before running your server CI scripts. Otherwise logs will be in red.)\nNow the notifying part is properly configured so we have to focus on automation ‚ô•‚ô•‚ô•. Here is the script file that does the automation.\ngit -C ../youtubeDownloader/react/ pull origin master sudo npm run build --prefix ../youtubeDownloader/react/ }  The first line pulling the github repository. But why ‚Äì C ? We are runing this script while not being in the relevant git directory. So we have to point git that this is the directory that should get updated and check the .git directory in the path mentioned. Also it is essential to save your github credentials to run the aboue. Otherwise a authentication step may be added. It makes automation harder.\ngit config credential.helper store }  The second one also does something similar. It points to the directory path to npm using ‚Äìprefix flag. ‚Äúnpm run build‚Äù is the script that we use to build react build directory. sudo may need to use to avoid permission issues.\nafter creating the above script file as ‚ÄúautoBuilder.sh‚Äù inside the server. Now we have to give sufficient permimissions\nchmod +x autoBuilder.sh }  Now we have a method to be notified and a script to automate. Now what. well a little linkage left. But how? If I simply say we need to find a way to run a shell script when a new request comes from Github. We can use ShellJs package in npm for that. It allowed us to run terminal command using Node. ShellJs API allows to run basic shell commands like cp, cd etc. Also we can execute a shell command like,\nshell.exec(\u0026quot;sudo ./autoBuilder.sh\u0026quot;); }  I developed a separate controller called ‚ÄúCI.js‚Äù for running that. Here is it.\nvar shell = require('shelljs'); exports.builder=function(res){ shell.echo('this is from shelljs Module'); //sending the response for github post request as it gets time out if we wait until automation. res.sendStatus(200); shell.exec(\u0026quot;sudo ./autoBuilder.sh\u0026quot;); } }  (I have placed my ‚ÄúautoBuilder.sh‚Äù inside the listening server directory root).\nThis is how my console seems when a I added a new ‚Äúreadme.md‚Äù file to the master.\nFew More things. In you use\nnode server.js  or\nnpm start  commands to start the node server of the application. It won‚Äôt adapt to the changes happen in the build file. So we have to take care of that too. We can use nodemon npm package to listen for the file changes on the server. It can be simply installed as follows.\nnpm install -g nodemon  First change the current directory to the server directory of the application. Then start a new window using screen. (Normally we use ‚Äúscreen‚Äù tool to start services on the server. it lets us to keep running the service while we doing something else on the server. here is a guideline from digitalOcean for screen tool) .\nscreen  nodemon server.js  Now press to leave the screen.\nCTRL + a + d  What‚Äôs More? This seems easy and simple. But there are few drawbacks with this approach. Yet we can address them efficiently.\nThe build that we deploy on the live environment is not properly tested.\n Building the build directory only on a test environment when a commit comes. Testing the build using ‚Äúnpm test‚Äù. (we can write scripts for testing under ‚Äútest‚Äù script in package.json) If it passes put deploy it to the production(simply coping), else reject. The server restarts each and every time when we are committing, even for minor commits! We can implement a method to filter out the commits notification and build for only specific commits.  We don‚Äôt manages the releases here. That is really bad practise to do if we are deploy something serious. Deployment automation tools like Ansible, Chef could help for these problems.\nThat‚Äôs it for this post. Hope you learned something out of this. Thanks for reading.\n","date":1489708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489708800,"objectID":"ea838813375db2d8602b6f3117fe21ef","permalink":"/post/ci-cd/","publishdate":"2017-03-17T00:00:00Z","relpermalink":"/post/ci-cd/","section":"post","summary":"CI/CD with Github Webhooks","tags":["CI/CD, Github, Codes Demo"],"title":"Continuous Integration and Deploying with Github Webhooks","type":"post"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"067123349696e864f6db6b64618bd76a","permalink":"/project/intervest/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/intervest/","section":"project","summary":"An example of using the in-built project page.","tags":["Consultant","SE"],"title":"Automated Document Processing System","type":"project"},{"authors":null,"categories":null,"content":"Conducting meetings for mobile aggregation with mobile network operators (Dialog, Mobitel, Airtel, Etisalat) in Sri Lanka on behalf of Ongoza.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"fb50be9576a0aa5a7b0b267de089f203","permalink":"/project/ongoza/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/ongoza/","section":"project","summary":"An example of using the in-built project page.","tags":["Consultant","BD"],"title":"Bussiness Developement","type":"project"},{"authors":null,"categories":null,"content":"This application was built for Natural Language Processing Centre of University of Moratuwa. The client had a very large data corpus and there were some erroneous tags for the words. A tag is basically the type of the word like abbreviation, full stop, adjectival noun, postpositions, pronouns etc (There are around 30 tags). It was required to change the tag according to the position of the word in a single sentence or change the tag of the selected word for the whole corpus. Usability and consistency were the main considerations of the application. So different methods were added to search the content like searching by word and searching by tag. The editor can be configured to and corpus with the same corpus conventions and the changes are consistently saved in the file system.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1054931b14cbbba34f08cf3a90ca592a","permalink":"/project/corpus/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/corpus/","section":"project","summary":"This application was built for Natural Language Processing Centre of University of Moratuwa. The client had a very large data corpus and there were some erroneous tags for the words.","tags":["Deep Learning"],"title":"Corpus Editor","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"5517c023c32a107327d2a68ddf5edbd1","permalink":"/project/transit/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/transit/","section":"project","summary":"An example of using the in-built project page.","tags":["SE","Tachyon"],"title":"Data Visualization Platform","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8dfc724d13492514de6bdc3837b7aa1c","permalink":"/project/directional/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/directional/","section":"project","summary":"An example of using the in-built project page.","tags":["Ciperlabs","SE"],"title":"Directional Speakers","type":"project"},{"authors":null,"categories":null,"content":"A neural network based intent classification system for a specific domain that has the capability to identify the intent of an uttered command in Sinhala language without converting to text. Two research papers were submitted. A mobile web app was developed for collecting voice samples.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9ab01f8c21bd6f59a405d68b676d5b4e","permalink":"/project/fyp/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/fyp/","section":"project","summary":"A neural network based intent classification system for a specific domain that has the capability to identify the intent of an uttered command in Sinhala language without converting to text. Two research papers were submitted. A mobile web app was developed for collecting voice samples.","tags":["Deep Learning","Machine Learning","NLP","RnD","Academic"],"title":"Domain specific intent classification system ...","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"64f9300454378e4e14b030ba51bcab55","permalink":"/project/zoomi/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/zoomi/","section":"project","summary":"An example of using the in-built project page.","tags":["SE","Tachyon"],"title":"ERP Backend Development","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"7d0ae3ce1f55be36bd7e236d92fa6ec2","permalink":"/project/intermind/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/intermind/","section":"project","summary":"An example of using the in-built project page.","tags":["SE","Consultant"],"title":"Software Deployment","type":"project"},{"authors":null,"categories":null,"content":"This application was built for Natural Language Processing Centre of University of Moratuwa. The client had a very large data corpus and there were some erroneous tags for the words. A tag is basically the type of the word like abbreviation, full stop, adjectival noun, postpositions, pronouns etc (There are around 30 tags). It was required to change the tag according to the position of the word in a single sentence or change the tag of the selected word for the whole corpus. Usability and consistency were the main considerations of the application. So different methods were added to search the content like searching by word and searching by tag. The editor can be configured to and corpus with the same corpus conventions and the changes are consistently saved in the file system.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"52bb183cfd6956f34610c24f6c0f2f21","permalink":"/project/transport/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/transport/","section":"project","summary":"This application was built for Natural Language Processing Centre of University of Moratuwa. The client had a very large data corpus and there were some erroneous tags for the words.","tags":["Deep Learning","Machine Learning","RnD","Academic"],"title":"Travel Behaviour Analytics for Public Transportation Services","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"48811bf06a854e86f56cb20ffa257d78","permalink":"/project/pleco/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/pleco/","section":"project","summary":"An example of using the in-built project page.","tags":["SE","Ciperlabs"],"title":"Unicode docx/pdf converter","type":"project"},{"authors":["Sudeepa Nadeeshan","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"An example journal article","type":"publication"},{"authors":["Sudeepa Nadeeshan"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"8c8e8c16ed2439aea158b65fcfbc1731","permalink":"/publication/domainspecific/","publishdate":"2013-07-01T00:00:00Z","relpermalink":"/publication/domainspecific/","section":"publication","summary":"Building an open domain automatic speech recognition(ASR) system can be accomplished by converting voice into text and performing a text classification on top of the converted text.","tags":["RnD","NLP","Machine Learning","Sinhala Speech Recognition","Feed Forward Neural Network","Speech recognition"],"title":"Domain Specific Intent Classification of Sinhala Speech Data","type":"publication"}]